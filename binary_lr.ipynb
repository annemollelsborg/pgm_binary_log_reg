{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Logistic Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Logistic Regression for Binary Classification using Pyro\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.infer import MCMC, NUTS, HMC, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import itertools\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal\n",
    "palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "# fix random generator seed (for reproducibility of results)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "data = pd.read_csv('train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1058, 53)\n",
      "(1058,)\n",
      "(1058,)\n"
     ]
    }
   ],
   "source": [
    "# Separate between features/inputs (X) and target/output variables (y)\n",
    "mat = data.values\n",
    "X = np.delete(mat, 1, axis=1)\n",
    "print(X.shape)\n",
    "y = mat[:, 1].astype(\"int\")\n",
    "print(y.shape)\n",
    "ind = mat[:,1].astype(\"int\")  #and get the indexes\n",
    "print(ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/z6bjwn212yg9839rhl52qvtm0000gn/T/ipykernel_7831/822795065.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  X = (X - X_mean) / X_std\n"
     ]
    }
   ],
   "source": [
    "# standardize input features\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 698\n",
      "num test: 360\n"
     ]
    }
   ],
   "source": [
    "train_perc = 0.66 # percentage of training data\n",
    "split_point = int(train_perc*len(y))\n",
    "perm = np.random.permutation(len(y)) # we also randomize the dataset\n",
    "ix_train = perm[:split_point]\n",
    "ix_test = perm[split_point:]\n",
    "X_train = X[ix_train,:]\n",
    "X_test = X[ix_test,:]\n",
    "y_train = y[ix_train]\n",
    "y_test = y[ix_test]\n",
    "print(\"num train: %d\" % len(y_train))\n",
    "print(\"num test: %d\" % len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulagranlund/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [ 4 15 20]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/paulagranlund/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [ 4 15 20]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create imputer (mean strategy works well for most numeric data)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit to training data and transform both train and test\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "true values: [0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1]\n",
      "Accuracy: 0.8583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulagranlund/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create and fit logistic regression model\n",
    "logreg = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto', C=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test set\n",
    "y_hat = logreg.predict(X_test)\n",
    "print(\"predictions:\", y_hat)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def model(X, n_cat, y=None):\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    beta = pyro.sample(\"beta\", dist.Normal(0., 1.).expand([n_features, 1]).to_event(2))\n",
    "    logits = X @ beta  # shape: (N, 1)\n",
    "\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logits.squeeze(-1)), obs=y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, n_cat, y=None):\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Sample one weight per feature â†’ beta has shape (D,)\n",
    "    beta = pyro.sample(\"beta\", dist.Normal(0., 1.).expand([n_features]).to_event(1))\n",
    "\n",
    "    logits = X @ beta  # shape: (N,)\n",
    "\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        pyro.sample(\"y\", dist.Bernoulli(logits=logits), obs=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/z6bjwn212yg9839rhl52qvtm0000gn/T/ipykernel_7831/2804050145.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train).float()\n",
      "/var/folders/1m/z6bjwn212yg9839rhl52qvtm0000gn/T/ipykernel_7831/2804050145.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train).float()\n",
      "/var/folders/1m/z6bjwn212yg9839rhl52qvtm0000gn/T/ipykernel_7831/2804050145.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test).float()\n",
      "/var/folders/1m/z6bjwn212yg9839rhl52qvtm0000gn/T/ipykernel_7831/2804050145.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test).float()\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] ELBO: 914.22\n",
      "[Step 1000] ELBO: 532.35\n",
      "[Step 2000] ELBO: 525.32\n",
      "[Step 3000] ELBO: 516.93\n",
      "[Step 4000] ELBO: 517.03\n",
      "[Step 5000] ELBO: 519.89\n",
      "[Step 6000] ELBO: 516.25\n",
      "[Step 7000] ELBO: 520.26\n",
      "[Step 8000] ELBO: 513.12\n",
      "[Step 9000] ELBO: 515.85\n",
      "[Step 10000] ELBO: 519.35\n",
      "[Step 11000] ELBO: 510.90\n",
      "[Step 12000] ELBO: 517.40\n",
      "[Step 13000] ELBO: 514.13\n",
      "[Step 14000] ELBO: 517.32\n",
      "[Step 15000] ELBO: 512.85\n",
      "[Step 16000] ELBO: 513.78\n",
      "[Step 17000] ELBO: 518.28\n",
      "[Step 18000] ELBO: 517.67\n",
      "[Step 19000] ELBO: 512.15\n",
      "[Step 20000] ELBO: 514.59\n",
      "[Step 21000] ELBO: 514.26\n",
      "[Step 22000] ELBO: 514.13\n",
      "[Step 23000] ELBO: 514.24\n",
      "[Step 24000] ELBO: 518.14\n",
      "[Step 25000] ELBO: 514.37\n",
      "[Step 26000] ELBO: 513.16\n",
      "[Step 27000] ELBO: 512.00\n",
      "[Step 28000] ELBO: 514.73\n",
      "[Step 29000] ELBO: 514.31\n",
      "[Step 30000] ELBO: 512.43\n",
      "[Step 31000] ELBO: 511.17\n",
      "[Step 32000] ELBO: 513.46\n",
      "[Step 33000] ELBO: 517.75\n",
      "[Step 34000] ELBO: 514.02\n",
      "[Step 35000] ELBO: 512.86\n",
      "[Step 36000] ELBO: 517.63\n",
      "[Step 37000] ELBO: 513.09\n",
      "[Step 38000] ELBO: 514.17\n",
      "[Step 39000] ELBO: 515.21\n"
     ]
    }
   ],
   "source": [
    "# Clear previous state\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define guide\n",
    "guide = AutoMultivariateNormal(model)\n",
    "\n",
    "# Optimizer and ELBO\n",
    "optimizer = ClippedAdam({\"lr\": 0.001})\n",
    "elbo = Trace_ELBO()\n",
    "svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Training loop\n",
    "n_steps = 40000\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(X_train, n_cat=None, y=y_train.float())\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"[Step {step}] ELBO: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 2D and 0D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m beta_mean \u001b[39m=\u001b[39m beta_samples\u001b[39m.\u001b[39mmean()  \u001b[39m# shape (D,)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m \u001b[39m# Predict probabilities\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m logits_test \u001b[39m=\u001b[39m X_test \u001b[39m@\u001b[39;49m beta_mean\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m probs_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(logits_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=8'>9</a>\u001b[0m y_pred \u001b[39m=\u001b[39m (probs_test \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mint()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 2D and 0D"
     ]
    }
   ],
   "source": [
    "# Posterior predictive\n",
    "samples = guide()\n",
    "beta_samples = samples['beta'].detach()\n",
    "beta_mean = beta_samples.mean()  # shape (D,)\n",
    "\n",
    "# Predict probabilities\n",
    "logits_test = X_test @ beta_mean\n",
    "probs_test = torch.sigmoid(logits_test)\n",
    "y_pred = (probs_test > 0.5).int()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (y_pred == y_test).float().mean()\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_samples shape: torch.Size([50])\n",
      "beta_mean shape: torch.Size([])\n",
      "X_test shape: torch.Size([360, 50])\n"
     ]
    }
   ],
   "source": [
    "print(\"beta_samples shape:\", beta_samples.shape)  # (num_samples, D)\n",
    "print(\"beta_mean shape:\", beta_mean.shape)        # (D,)\n",
    "print(\"X_test shape:\", X_test.shape)              # (N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got input (360), mat (360x50), vec (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W3sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m beta_mean \u001b[39m=\u001b[39m beta_samples\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W3sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m \u001b[39m# Predict probabilities\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W3sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m logits_test \u001b[39m=\u001b[39m X_test \u001b[39m@\u001b[39;49m beta_mean\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W3sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m probs_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(logits_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W3sdW50aXRsZWQ%3D?line=8'>9</a>\u001b[0m y_pred \u001b[39m=\u001b[39m (probs_test \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mint()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got input (360), mat (360x50), vec (1)"
     ]
    }
   ],
   "source": [
    "# Visualize posterior distributions of betas\n",
    "for d in range(beta_samples.shape[0]):\n",
    "    sns.histplot(beta_samples[d].squeeze(), kde=True, alpha=0.3)\n",
    "plt.title(\"Posterior distributions of beta coefficients\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
